{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building deep learning models with keras\n",
    "You'll use the Keras library to build deep learning models for classification. By the end of the chapter, you'll have all the tools necessary to build deep neural networks.\n",
    "\n",
    "This demo is a jupyter notebook, i.e. intended to be run step by step.\n",
    "\n",
    "Author: Eric Einspänner\n",
    "\n",
    "First version: 6th of July 2023\n",
    "\n",
    "\n",
    "Copyright 2023 Clinic of Neuroradiology, Magdeburg, Germany\n",
    "\n",
    "License: Apache-2.0\n",
    "\n",
    "*This notebook is inspired by: https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Of Contents\n",
    "1. [Intial set-up](#initial-set-up)\n",
    "2. [Load Data](#load-data)\n",
    "    - [The Dataset](#the-dataset)\n",
    "    - [Details](#details)\n",
    "3. [Define Keras Model](#define-keras-model)\n",
    "    - [Summarize model](#summarize-model)\n",
    "    - [Visualize model](#visualize-model)\n",
    "4. [Compile Keras Model](#compile-keras-model)\n",
    "5. [Fit Keras Model](#fit-keras-model)\n",
    "6. [Evaluate Keras Model](#evaluate-keras-model)\n",
    "    - [Visualize Model Training History](#visualize-model-training-history-in-keras)\n",
    "7. [Save a Keras Model](#save-a-keras-model)\n",
    "8. [Load a Keras Model](#load-a-keras-model)\n",
    "9. [Make Predictions](#make-predictions)\n",
    "10. [Changing Optimization Parameters](#changing-optimization-parameters)\n",
    "11. [Try Yourself](#try-yourself)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Set-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure figures appears inline and animations works\n",
    "# Edit this to \"\"%matplotlib notebook\" when using the \"classic\" jupyter notebook interface\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from keras.utils import plot_model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_render_plot():\n",
    "    '''Custom function to simplify common formatting operations for exercises. Operations include: \n",
    "    1. Turning off axis grids.\n",
    "    2. Calling `plt.tight_layout` to improve subplot spacing.\n",
    "    3. Calling `plt.show()` to render plot.'''\n",
    "    fig = plt.gcf()\n",
    "    for ax in fig.axes:\n",
    "        ax.legend(loc='center right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "We will use the `NumPy` library to load the dataset and two classes from the Keras library to define the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "In this Keras tutorial, we will use the Pima Indians onset of diabetes dataset. This is a standard machine learning dataset from the UCI Machine Learning repository. It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years.\n",
    "\n",
    "As such, it is a binary classification problem (onset of diabetes as 1 or not as 0). All of the input variables that describe each patient are numerical. This makes it easy to use directly with neural networks that expect numerical input and output values and is an ideal choice for our first neural network in Keras.\n",
    "\n",
    "The dataset is available here:\n",
    "\n",
    "- [Dataset CSV File (pima-indians-diabetes.csv)](/notebooks/DeepLearning/Data/pima-indians-diabetes.csv)\n",
    "- [Dataset Details](/notebooks/DeepLearning/Data/details_pima-indians-diabetes.txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'Data/pima-indians-diabetes.csv'\n",
    "\n",
    "# load the dataset\n",
    "dataset = np.loadtxt(data_path, delimiter=',')\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details\n",
    "There are eight input variables and one output variable (the last column). You will be learning a model to map rows of input variables (X) to an output variable (y), which is often summarized as *y = f(X)*.\n",
    "\n",
    "The variables can be summarized as follows:\n",
    "\n",
    "Input Variables (X):\n",
    "\n",
    "- Number of times pregnant\n",
    "- Plasma glucose concentration at 2 hours in an oral glucose tolerance test\n",
    "- Diastolic blood pressure (mm Hg)\n",
    "- Triceps skin fold thickness (mm)\n",
    "- 2-hour serum insulin (mu U/ml)\n",
    "- Body mass index (weight in kg/(height in m)^2)\n",
    "- Diabetes pedigree function\n",
    "- Age (years)\n",
    "\n",
    "Output Variables (y):\n",
    "\n",
    "- Class variable (0 or 1)\n",
    "\n",
    "Once the CSV file is loaded into memory, you can split the columns of data into input and output variables.\n",
    "\n",
    "The data will be stored in a 2D array where the first dimension is rows and the second dimension is columns, e.g., [rows, columns].\n",
    "\n",
    "You can split the array into two arrays by selecting subsets of columns using the standard NumPy slice operator or `:`. You can select the first eight columns from index 0 to index 7 via the slice 0:8. We can then select the output column (the 9th variable) via index 8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras Model\n",
    "Models in Keras are defined as a sequence of layers.\n",
    "\n",
    "We create a Sequential model and add layers one at a time until we are happy with our network architecture.\n",
    "\n",
    "The first thing to get right is to ensure the input layer has the correct number of input features. This can be specified when creating the first layer with the `input_shape` argument and setting it to (8,) for presenting the eight input variables as a vector.\n",
    "\n",
    "How do we know the number of layers and their types?\n",
    "\n",
    "This is a tricky question. There are heuristics that you can use, and often the best network structure is found through a process of trial and error experimentation (I explain more about this here). Generally, you need a network large enough to capture the structure of the problem.\n",
    "\n",
    "In this example, let’s use a fully-connected network structure with three layers.\n",
    "\n",
    "Fully connected layers are defined using the Dense class. You can specify the number of neurons or nodes in the layer as the first argument and the activation function using the activation argument.\n",
    "\n",
    "Also, you will use the rectified linear unit activation function referred to as ReLU on the first two layers and the Sigmoid function in the output layer.\n",
    "\n",
    "It used to be the case that Sigmoid and Tanh activation functions were preferred for all layers. These days, better performance is achieved using the ReLU activation function. Using a sigmoid on the output layer ensures your network output is between 0 and 1 and is easy to map to either a probability of class 1 or snap to a hard classification of either class with a default threshold of 0.5.\n",
    "\n",
    "You can piece it all together by adding each layer:\n",
    "\n",
    "The model expects rows of data with 8 variables (the `input_shape=(8,)` argument).\n",
    "- The first hidden layer has 12 nodes and uses the relu activation function.\n",
    "- The second hidden layer has 8 nodes and uses the relu activation function.\n",
    "- The output layer has one node and uses the sigmoid activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the keras model\n",
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(12, input_shape=(8,), activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Model\n",
    "Keras provides a way to summarize a model.\n",
    "\n",
    "The summary is textual and includes information about:\n",
    "- The layers and their order in the model.\n",
    "- The output shape of each layer.\n",
    "- The number of parameters (weights) in each layer.\n",
    "- The total number of parameters (weights) in the model.\n",
    "- The summary can be created by calling the `summary()` function on the model that returns a string that in turn can be printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model\n",
    "The summary is useful for simple models, but can be confusing for models that have multiple inputs or outputs.\n",
    "\n",
    "Keras also provides a function to create a plot of the network neural network graph that can make more complex models easier to understand.\n",
    "\n",
    "The `plot_model()` function in Keras will create a plot of your network. This function takes a few useful arguments:\n",
    "- model: (required) The model that you wish to plot.\n",
    "- to_file: (required) The name of the file to which to save the plot.\n",
    "- show_shapes: (optional, defaults to False) Whether or not to show the output shapes of each layer.\n",
    "- show_layer_names: (optional, defaults to True) Whether or not to show the name for each layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file='Images/model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Keras Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is defined, you can compile it.\n",
    "\n",
    "Compiling the model uses the efficient numerical libraries under the covers (the so-called backend) such as Theano or TensorFlow. The backend automatically chooses the best way to represent the network for training and making predictions to run on your hardware, such as CPU, GPU, or even distributed.\n",
    "\n",
    "When compiling, you must specify some additional properties required when training the network. Remember training a network means finding the best set of weights to map inputs to outputs in your dataset.\n",
    "\n",
    "You must specify the loss function to use to evaluate a set of weights, the optimizer used to search through different weights for the network, and any optional metrics you want to collect and report during training.\n",
    "\n",
    "In this case, use cross entropy as the loss argument. This loss is for a binary classification problems and is defined in Keras as “binary_crossentropy“.\n",
    "\n",
    "We will define the optimizer as the efficient stochastic gradient descent algorithm “adam“. This is a popular version of gradient descent because it automatically tunes itself and gives good results in a wide range of problems.\n",
    "\n",
    "Finally, because it is a classification problem, you will collect and report the classification accuracy defined via the metrics argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Keras Model\n",
    "ou have defined your model and compiled it to get ready for efficient computation.\n",
    "\n",
    "Now it is time to execute the model on some data.\n",
    "\n",
    "You can train or fit your model on your loaded data by calling the fit() function on the model.\n",
    "\n",
    "Training occurs over epochs, and each epoch is split into batches.\n",
    "- Epoch: One pass through all of the rows in the training dataset\n",
    "- Batch: One or more samples considered by the model within an epoch before weights are updated\n",
    "\n",
    "One epoch comprises one or more batches, based on the chosen batch size, and the model is fit for many epochs.\n",
    "\n",
    "The training process will run for a fixed number of epochs (iterations) through the dataset that you must specify using the epochs argument. You must also set the number of dataset rows that are considered before the model weights are updated within each epoch, called the batch size, and set using the `batch_size` argument.\n",
    "\n",
    "This problem will run for a small number of epochs (150) and use a relatively small batch size of 10.\n",
    "\n",
    "These configurations can be chosen experimentally by trial and error. You want to train the model enough so that it learns a good (or good enough) mapping of rows of input data to the output classification. The model will always have some error, but the amount of error will level out after some point for a given model configuration. This is called model convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the keras model on the dataset\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Keras Model\n",
    "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset.\n",
    "\n",
    "This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy), but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset using the `evaluate()` function and pass it the same input and output used to train the model.\n",
    "\n",
    "This will generate a prediction for each input and output pair and collect scores, including the average loss and any metrics you have configured, such as accuracy.\n",
    "\n",
    "The `evaluate()` function will return a list with two values. The first will be the loss of the model on the dataset, and the second will be the accuracy of the model on the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the keras model\n",
    "scores = model.evaluate(X, y)\n",
    "print('Accuracy: %.2f' % (scores[1]))\n",
    "print('Loss: %.2f' % (scores[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Model Training History in Keras\n",
    "Keras provides the capability to register callbacks when training a deep learning model.\n",
    "\n",
    "One of the default callbacks registered when training all deep learning models is the History callback. It records training metrics for each epoch. This includes the loss and the accuracy (for classification problems) and the loss and accuracy for the validation dataset if one is set.\n",
    "\n",
    "The history object is returned from calls to the fit() function used to train the model. Metrics are stored in a dictionary in the history member of the object returned.\n",
    "\n",
    "The example collects the history returned from training the model and creates two charts:\n",
    "- A plot of accuracy on the training and validation datasets over training epochs\n",
    "- A plot of loss on the training and validation datasets over training epochs\n",
    "\n",
    "*One more thing:*\n",
    "\n",
    "With the `validation_split` option it is possible to split our data into two different datasets: one for training and the other for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X, y, validation_split=0.33, epochs=150, batch_size=10, verbose=0)\n",
    "\n",
    "# list all data in history\n",
    "print(history.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy and loss\n",
    "fig, axes = plt.subplots(2, 1, sharex=True)\n",
    "axes[0].plot(history.history['accuracy'], label='accuracy')\n",
    "axes[0].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "axes[0].set_title('model accuracy', fontweight =\"bold\")\n",
    "axes[0].set_ylabel('accuracy')\n",
    "axes[0].set_ylim([0, 1])\n",
    "\n",
    "axes[1].plot(history.history['loss'], label='loss')\n",
    "axes[1].plot(history.history['val_loss'], label='val_loss')\n",
    "axes[1].set_title('model loss', fontweight =\"bold\")\n",
    "axes[1].set_ylabel('val_loss')\n",
    "axes[1].set_xlabel('epoch')\n",
    "axes[1].set_ylim([0, max(history.history['loss'])])\n",
    "format_and_render_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots can provide an indication of useful things about the training of the model, such as:\n",
    "- Its speed of convergence over epochs (slope)\n",
    "- Whether the model may have already converged (plateau of the line)\n",
    "- Whether the mode may be over-learning the training data (inflection for validation line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save a Keras Model\n",
    "You can save your model by calling the save() function on the model and specifying the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model and architecture to single file\n",
    "model.save(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Keras Model\n",
    "Your saved model can then be loaded later by calling the load_model() function and passing the filename. The function returns the model with the same architecture and weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = load_model('model.h5')\n",
    "# summarize model.\n",
    "model.summary()\n",
    "\n",
    "# evaluate the model\n",
    "score = model.evaluate(X, y, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "The number one question I get asked is: *After I train my model, how can I use it to make predictions on new data?*\n",
    "\n",
    "You can adapt the above example and use it to generate predictions on the training dataset, pretending it is a new dataset you have not seen before.\n",
    "\n",
    "Making predictions is as easy as calling the `predict()` function on the model. You are using a sigmoid activation function on the output layer, so the predictions will be a probability in the range between 0 and 1. You can easily convert them into a crisp binary prediction for this classification task by rounding them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make probability predictions with the model\n",
    "predictions = (model.predict(X) > 0.5).astype(int)\n",
    "\n",
    "# summarize the first 5 cases\n",
    "for i in range(5):\n",
    "    print('%s => %d (expected %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing Optimization Parameters\n",
    "We'll now try optimizing a model at different learning rates. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of learning rates: lr_to_test\n",
    "lr_to_test = [.000001, 0.01, 1]\n",
    "\n",
    "# Loop over learning rates\n",
    "for lr in lr_to_test:\n",
    "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
    "    \n",
    "    # Build new model to test, unaffected by previous models\n",
    "    model = get_model()\n",
    "    \n",
    "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
    "    my_optimizer = Adam(learning_rate=lr)\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=my_optimizer, loss='binary_crossentropy')\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try Yourself\n",
    "Tune the Model. Change the configuration of the model or training process and see if you can improve the performance of the model, e.g., achieve better than 76% accuracy.\n",
    "Some hints:\n",
    "- Add layers\n",
    "- Find the optimal learning rate\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bootcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
